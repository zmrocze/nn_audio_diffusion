{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"benjamin-paine/free-music-archive-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where genres contain both 70 and 4: Dataset({\n",
      "    features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
      "    num_rows: 929\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ds = ds.with_format(\"polars\")\n",
    "# import polars\n",
    "# import polars.datatypes\n",
    "\n",
    "def has_all_genres(examples, genre_ids):\n",
    "  return all(g in examples['genres'] for g in genre_ids)\n",
    "\n",
    "def has_any_genres(examples, genre_ids):\n",
    "  return any(g in examples['genres'] for g in genre_ids)\n",
    "\n",
    "\n",
    "fds = ds['train'].filter(lambda x: has_any_genres(x, [12, 26, 58, 25]))\n",
    "\n",
    "# ind = ds['train'][:].loc[:, (polars_column.map_elements(lambda x : all( g in [4, 26] for g in x) , return_dtype=polars.datatypes.Boolean) )]\n",
    "# indices = [i for i, genres in enumerate(polars_column) if genres is not None and 70 in genres and 4 in genres]\n",
    "print(f\"Indices where genres contain both 70 and 4: {fds}\")\n",
    "# .filter(lambda x: print(x) == None).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique genres: 21\n",
      "All unique genres: [6, 15, 23, 26, 27, 28, 39, 43, 45, 46, 51, 58, 60, 65, 73, 74, 96, 121, 136, 156, 158]\n"
     ]
    }
   ],
   "source": [
    "# Get all unique genres from the dataset\n",
    "all_genres = set()\n",
    "for example in fds['genres']:\n",
    "  if example:\n",
    "    all_genres.update(example)\n",
    "\n",
    "print(f\"Number of unique genres: {len(all_genres)}\")\n",
    "print(\"All unique genres:\", sorted(list(all_genres)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique artists in filtered dataset: 212\n",
      "Artists: ['20lb Sounds', 'Ai Aso', 'Albin Andersson', 'Alec K. Redfearn & the Eyesores', 'Alexander Turnquist', 'Allysen Callery', 'Alyiann', 'Amitron_7', 'Anamoli', 'Andrea Tomasi', 'Andreas Woiger', 'Anima', 'Beat Culture', 'Beat Doctor', 'Ben von Wildenhaus', 'Big Blood', 'Bill Borman', 'Bird of Youth', 'Boca Chica', \"Bonnie 'Prince' Billy\", 'Brett Saxon', 'Bridget St John', 'Brigid Mae Power', 'Brock Tyler', 'Broke For Free', 'Cal Folger Day', 'Carter Behnke', 'Cath and Phil Tyler', 'Charles Manson', 'Chuck Johnson', 'Cityzens Were Here', 'Colby Maddox', 'Connie Acher & Blind Drunk John', 'Connie Acher & Jelly', 'Curtis Eller', 'D SMILEZ', 'Dave Kent', 'David Rovics', 'David Szesztay', 'Debby Schwartz', 'Delgarma', 'Derek Clegg', 'Dick Whyte and his Golden Guitar', \"Digi G'Alessio\", 'Dora Bleu', 'Ed Askew', 'Ed Trickett', 'Entertainment for the Braindead', 'Eric Carbonara', 'Eric Lindley', 'Eszter Balint', 'Fast Freddie Fourier and the Transforms', 'Finn Anderson', 'Flor Braier', 'Foghorn Stringband', 'Frame', 'Fursaxa', 'Gary Higgins', 'Ghostly Dust Machine', 'Gilo', 'Gilo feat. As Metralhadoras', 'Goddess', 'Gorowski', 'Graeme Mark', 'Green Like July', 'Hall Of Fame', 'Hope For Agoldensummer', 'Howie & Ann Mitchell', 'Howie Mitchell', 'Howie Mitchell & Charlotte Williams', 'Howie Mitchell & Ruth Meyer', 'Howie Mitchell, Ed Trickett', 'Hudson', 'Humble Tripe', 'I, Cactus', 'Illusory Scapes', 'Itasca', 'JMD v Kube', 'Jacket Thor', 'Jackson Grimm', 'Jahzzar', 'James Beaudreau', 'James Blackshaw', 'Jan Grünfeld', 'Janet Bean', 'Jay Martinez', 'Jenny O.', 'Joan Shelley', 'Joanna Sternberg', 'John Wort Hannam', 'Jonathan Coulton', 'Josh Woodward', 'Julie Byrne', 'Jupiter Makes Me Scream', 'Just Plain Ant', 'Kakurenbo', 'Kalli', 'Katy Kirby', 'Keaton Henson', 'Kelly Latimore', 'Kevin MacLeod', 'Kink Slap Simian', 'Koalips', 'La Cantine Boga', 'Larkin Grimm', 'Least Carpet', 'Leif Vollebek', 'Lille', 'Linda Draper', 'Lior', 'Lonely Faction', 'Love Story In Blood Red', 'Marc Burt', 'Mariana Päraway', 'Marissa Nadler', 'Mark Fosson', 'Marquice Turner', 'Mary Caroline', 'Mary Halvorson and Jessica Pavone', 'Matt Bauer', 'Matt Sowell', 'Matúš Novanský', 'Meaner Pencil', 'Meg Baird', 'Melaena Cadiz', 'Melissa Laveaux', 'Mia Doi Todd', 'Mice Parade', 'Michael Chapman', 'Michael Holt', 'Michett', 'Mike B. Fort', 'MindsEye', 'Misay Day', 'Montana Skies', 'Morsa', 'Mount Eerie', 'Mr. & Mrs. Smith', 'My Bubba & Mi', 'Myriam Gendron', 'Mystery Mammal', 'Nalle', 'Nate Maingard', 'Nick Corbo', 'Nick Jaina', 'Not From This World', 'Oblivian Substanshall', 'Old Town School Of Folk Music (Various)', 'Online Funkstream', 'Ostin', 'P.G. Six', 'Pepper Coat', 'Pete Lund', 'Peter Biedermann', 'Philippa Dowding and Allister Thompson', 'Phish Funk', 'Pierce', 'Plusplus', 'Project 5am', 'Project 5am (Feat. DJ Whodini)', 'Psychadelik Pedestrian', 'Rachel Ries', 'Red Trees', 'Redmann', 'Richard Wolff', 'Risey', 'Robert Farmer', 'Robin Grey', 'Robin Mitchell', 'Robo', 'Sam Moss', 'Samuel Vas Y', 'Sandy & Caroline Paton', 'Sea Offs', 'Sean Fournier', 'Seaven Teares', 'Shatterfreak', 'Six Organs of Admittance', 'Sondra Sun-Odeon', 'Space Invaders v Psychadelik Pedestrian', 'Spires That in the Sunset Rise', 'Srch Party', 'Sro', 'Stealing Orchestra', 'Steffen Basho-Junghans', 'Stephen Bartolomei', 'Stærosaurus', 'Sycamore Drive', 'The Bribe and the Mockery', 'The Chapin Sisters', 'The Golden Dawn', 'The John Barry Conception', 'The Twin Atlas', 'The Wingdale Community Singers', 'This Mess is Mine', 'Three Jolly Rogues', 'Tommy Jay', 'Twi the Humble Feather', 'Uke of Spaces Corners', 'Unique Comme Tout le Monde', 'Vio/Mire', 'Wellington Sea Shanty Society', 'Whitman', 'Wildlight', 'Windmill', 'Yair Yona', 'Zach Fleury', 'Zachary Cale', 'kesson shoujo', 'mo-seph', 'spinningmerkaba', \"talons'\"]\n"
     ]
    }
   ],
   "source": [
    "# Get all unique artists from the filtered dataset\n",
    "artists = set()\n",
    "for example in fds['artist']:\n",
    "  artists.add(example)\n",
    "\n",
    "print(f\"Number of unique artists in filtered dataset: {len(artists)}\")\n",
    "print(\"Artists:\", sorted(list(artists)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fds.save_to_disk(\"filtered_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train artists: 148\n",
      "Val artists: 31\n",
      "Test artists: 33\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Get list of all artists and set random seed for reproducibility\n",
    "artist_list = list(artists)\n",
    "random.seed(42)\n",
    "random.shuffle(artist_list)\n",
    "\n",
    "n_artist = len(artist_list)\n",
    "train_size = int(0.7 * n_artist)\n",
    "val_size = int(0.15 * n_artist)\n",
    "\n",
    "train_artists = set(artist_list[:train_size])\n",
    "val_artists = set(artist_list[train_size:train_size + val_size])\n",
    "test_artists = set(artist_list[train_size + val_size:])\n",
    "\n",
    "print(f\"Train artists: {len(train_artists)}\")\n",
    "print(f\"Val artists: {len(val_artists)}\")\n",
    "print(f\"Test artists: {len(test_artists)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split size: 686\n",
      "Val split size: 119\n",
      "Test split size: 124\n",
      "Total: 929 (original fds: 929)\n"
     ]
    }
   ],
   "source": [
    "# Split the filtered dataset (fds) based on artist splits\n",
    "train_split = fds.filter(lambda x: x['artist'] in train_artists)\n",
    "val_split = fds.filter(lambda x: x['artist'] in val_artists)\n",
    "test_split = fds.filter(lambda x: x['artist'] in test_artists)\n",
    "\n",
    "print(f\"Train split size: {len(train_split)}\")\n",
    "print(f\"Val split size: {len(val_split)}\")\n",
    "print(f\"Test split size: {len(test_split)}\")\n",
    "print(f\"Total: {len(train_split) + len(val_split) + len(test_split)} (original fds: {len(fds)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 686\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "filtered_ds = DatasetDict({\"train\": train_split, \n",
    "                                 \"validation\": val_split, \n",
    "                                 \"test\": test_split})\n",
    "# filtered_ds['genres'] = Dataset.from_dict({\"any\": [12, 26, 58, 25]})\n",
    "filtered_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "# from prefigure.prefigure import push_wandb_config\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torchaudio\n",
    "import torchtune.training\n",
    "import wandb\n",
    "import librosa\n",
    "import torchtune\n",
    "from datasets import load_from_disk\n",
    "from peft import LoraConfig, inject_adapter_in_model\n",
    "from dataclasses import dataclass\n",
    "# import torch_audiomentations as taug\n",
    "import torchaudio as ta\n",
    "import audiomentations as aug\n",
    "import audiomentations as taug\n",
    "import random\n",
    "# import diffusers\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning.callbacks import RichProgressBar\n",
    "import numpy as np\n",
    "\n",
    "models_map = {\n",
    "  \"glitch-440k\": {\n",
    "    'sample_rate': 48000,\n",
    "    'sample_size': 65536\n",
    "    },\n",
    "  \"jmann-small-190k\": {\n",
    "    'sample_rate': 48000,\n",
    "    'sample_size': 65536\n",
    "    },\n",
    "  \"jmann-large-580k\": {\n",
    "    'sample_rate': 48000,\n",
    "    'sample_size': 131072\n",
    "    },\n",
    "  \"maestro-150k\": {\n",
    "    'sample_rate': 16000,\n",
    "    'sample_size': 65536\n",
    "    },\n",
    "  \"unlocked-250k\": {\n",
    "  'sample_rate': 16000,\n",
    "  'sample_size': 65536\n",
    "  },\n",
    "  \"honk-140k\": {\n",
    "    'sample_rate': 16000,\n",
    "    'sample_size': 65536\n",
    "    },\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    model_name = \"unlocked-250k\"\n",
    "    data_path = \"./filter_fma_rock\"\n",
    "    sample_size = models_map[model_name]['sample_size']\n",
    "    sample_rate = models_map[model_name]['sample_rate']\n",
    "    data_loader_num_workers = 4\n",
    "    batch_size = 32\n",
    "    # eval_batch_size = 16  # how many images to sample during evaluation\n",
    "    gradient_accumulation_steps = 1\n",
    "    lora_rank = 16  # the rank of the LoRA layers1\n",
    "    lora_alpha = 16 # scaling factor, which seems hardly necessary\n",
    "    lr = 4e-5\n",
    "    lr_warmup_steps = 5 # epochs\n",
    "    num_cycles=0.5  # cosine annealing cycles\n",
    "    num_epochs = 100\n",
    "    # save_image_epochs = 10\n",
    "    save_model_epochs = 30\n",
    "    demo_every = 5\n",
    "    n_samples = 10\n",
    "    # save_demo = True\n",
    "    demo_save_path = './demo_songs'\n",
    "    ckpt_load_path = None # 'best', 'last', <path]>\n",
    "    wandb_log_model = 'all'\n",
    "    check_val_every_n_epoch = 5\n",
    "    # mixed_precision = \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir = f\"ddim-lora-{model_name}\"  # the model name locally and on the HF Hub\n",
    "    name = f\"ddim-lora-{model_name}-{random.randint(0, 1000)}\"  # the name of the wandb run\n",
    "    project_name = \"nn-audio-diffusion\"\n",
    "    save_path = f'{name}-ckpt'\n",
    "    # overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
    "    # seed = 42\n",
    "\n",
    "config = TrainingConfig()\n",
    "\n",
    "def optional(x, bool):\n",
    "  if bool:\n",
    "    return [x]\n",
    "  else:\n",
    "    return []\n",
    "\n",
    "# samples_per_s = 44100\n",
    "def audio_augmentations(sample_size, use_train_augs = False):\n",
    "  output_type = 'tensor'\n",
    "  augmentation = aug.Compose(\n",
    "    transforms=\n",
    "      optional( # can change length, therefore first\n",
    "        aug.RepeatPart(mode=\"insert\", p=0.8)\n",
    "        , use_train_augs\n",
    "      )\n",
    "      +\n",
    "      [aug.AdjustDuration(duration_samples=sample_size, p=1.0)]\n",
    "      +\n",
    "      optional(\n",
    "        aug.TimeStretch(\n",
    "            min_rate=0.8,\n",
    "            max_rate=1.25,\n",
    "            leave_length_unchanged=True,\n",
    "            p=0.8\n",
    "        ),\n",
    "        use_train_augs)\n",
    "      + optional(\n",
    "        aug.OneOf(\n",
    "          transforms=[\n",
    "              taug.LowPassFilter(\n",
    "                  min_cutoff_freq=500.0,\n",
    "                  max_cutoff_freq=2000.0,\n",
    "                  p=1.0,\n",
    "                  # output_type=output_type\n",
    "              ),\n",
    "              taug.HighPassFilter(\n",
    "                  min_cutoff_freq=100.0,\n",
    "                  max_cutoff_freq=2400.0,\n",
    "                  p=1.0,\n",
    "                  # output_type=output_type\n",
    "              ),\n",
    "              taug.BandPassFilter(\n",
    "                  # min_center_frequency=200.0,\n",
    "                  # max_center_frequency=4000.0,\n",
    "                  min_center_freq=200.0,\n",
    "                  max_center_freq=4000.0,\n",
    "                  p=1.0,\n",
    "                  # output_type=output_type\n",
    "              ),\n",
    "              aug.ClippingDistortion(min_percentile_threshold=0,\n",
    "                  max_percentile_threshold=30,\n",
    "                  p=0.5),\n",
    "              # aug.Limiter(\n",
    "              #   min_threshold_db=-16.0,\n",
    "              #   max_threshold_db=0.0,\n",
    "              #   threshold_mode=\"relative_to_signal_peak\",\n",
    "              #   p=1.0,\n",
    "              # ), # todo: cylimiter dependency problem\n",
    "              taug.PitchShift(\n",
    "                # min_transpose_semitones=-5.0,\n",
    "                # max_transpose_semitones=5.0,\n",
    "                min_semitones=-5.0,\n",
    "                max_semitones=5.0,\n",
    "                # sample_rate=sample_rate,\n",
    "                p=1.0,\n",
    "                # output_type=output_type\n",
    "              ),\n",
    "          ],\n",
    "          p=1.0\n",
    "        ),\n",
    "        use_train_augs)\n",
    "      +\n",
    "      optional(\n",
    "        aug.SomeOf( transforms=[\n",
    "          aug.TanhDistortion(\n",
    "            min_distortion=0.01,\n",
    "            max_distortion=0.15,\n",
    "            p=0.8\n",
    "          )\n",
    "          # ,\n",
    "          # aug.RepeatPart(mode=\"insert\", p=0.8),\n",
    "        ],\n",
    "        num_transforms=(0,None),\n",
    "        ),\n",
    "        use_train_augs)\n",
    "      +\n",
    "      optional(\n",
    "        aug.Gain(\n",
    "          min_gain_db=-15.0,\n",
    "          max_gain_db=5.0,\n",
    "          # p=0.5,\n",
    "          p=0.5,\n",
    "        )\n",
    "        , use_train_augs)\n",
    "      +\n",
    "      # optional(\n",
    "      #   taug.ShuffleChannels(p=0.5, \n",
    "      #     # output_type=output_type\n",
    "      #     ),\n",
    "      #   use_train_augs)\n",
    "      # +\n",
    "      \n",
    "      # taug.OneOf(transforms=[\n",
    "      #   taug.PeakNormalization(apply_to=\"only_too_loud_sounds\", p=1.0, output_type=output_type),\n",
    "        # [taug.PeakNormalization(apply_to=\"all\", p=1.0\n",
    "        #                         # , output_type=output_type\n",
    "        #                         )]\n",
    "        [aug.Normalize(apply_to=\"only_too_loud_sounds\", p=1.0)]\n",
    "      # ], output_type=output_type),\n",
    "      +\n",
    "      optional(\n",
    "        aug.PolarityInversion(p=0.5),\n",
    "      use_train_augs)\n",
    "  )\n",
    "\n",
    "  # def transforms(examples):\n",
    "  #   # print(len(examples['audio']), \"audio samples\")\n",
    "  #   x = [augmentation(aud['array'], sample_rate=aud['sampling_rate']) for aud in examples['audio']]\n",
    "  #   examples[\"audio\"] = x\n",
    "  #   print(\"Transformed audio samples:\", [aud.shape for aud in examples['audio']])\n",
    "  #   return examples\n",
    "  \n",
    "  # return transforms, augmentation\n",
    "  return augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = audio_augmentations(config.sample_size, config.sample_rate, use_train_augs=True)\n",
    "# aug(np.zeros((config.sample_size,)), sample_rate=config.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "config.data_loader_num_workers = 1\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "  def __init__(self, hf_dataset, transform=None):\n",
    "    self.dataset = hf_dataset\n",
    "    self.transform = transform\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    audio = item['audio']['array']\n",
    "    # if self.transform:\n",
    "    print(\"sampling_rate: \", item['audio']['sampling_rate'])\n",
    "    audio_tensor = self.transform(audio, sample_rate=item['audio']['sampling_rate'])\n",
    "    audio_tensorr = torch.tensor(audio_tensor, dtype=torch.float32)\n",
    "    print(\"transform:\", audio_tensor.shape)\n",
    "    print(\"tensor\", audio_tensorr.shape)\n",
    "    return audio_tensor\n",
    "\n",
    "def make_dataloaders(data, config=config):\n",
    "  sample_size=config.sample_size\n",
    "  # sample_rate=config.sample_rate\n",
    "  num_workers=config.data_loader_num_workers\n",
    "  batch_size=config.batch_size\n",
    "  test_transformation = audio_augmentations(sample_size, use_train_augs = False)\n",
    "  train_dataset = MusicDataset(data['train'], transform=audio_augmentations(sample_size, use_train_augs = True))\n",
    "  val_dataset = MusicDataset(data['validation'], transform=test_transformation)\n",
    "  test_dataset = MusicDataset(data['test'], transform=test_transformation)\n",
    "\n",
    "  dl = lambda ds: torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "  return {\n",
    "    'train': dl(train_dataset),\n",
    "    'validation': dl(val_dataset),\n",
    "    'test': dl(test_dataset),\n",
    "    'genres': data['genres'],\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 686\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "    genres: Dataset({\n",
       "        features: ['any'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_data = load_from_disk(\"./rock_dataset_resampled\")\n",
    "this_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "daloaders = make_dataloaders(this_data, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling_rate:  16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/core/transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "torch.Size([32, 65536])\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])torch.Size([32, 65536])\n",
      "\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "torch.Size([32, 65536])\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform:torch.Size([32, 65536])\n",
      " (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "torch.Size([32, 65536])\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  16000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[130]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdaloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/multiprocessing/connection.py:256\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/multiprocessing/connection.py:423\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/multiprocessing/connection.py:1118\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1115\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1117\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1118\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for x in daloaders['train']:\n",
    "  print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n"
     ]
    }
   ],
   "source": [
    "for x in filtered_ds['train']:\n",
    "  if x[\"audio\"][\"sampling_rate\"] != 44100:\n",
    "    print(x[\"audio\"][\"sampling_rate\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(data, config=config):\n",
    "  sample_size=config.sample_size\n",
    "  # config.sample_rate\n",
    "  # num_workers=config.data_loader_num_workers\n",
    "  # batch_size=config.batch_size\n",
    "  trs = lambda audio, sample_rate: torchaudio.functional.resample(\n",
    "    waveform = torch.tensor(audio, dtype=torch.float32), \n",
    "    orig_freq = sample_rate, \n",
    "    new_freq=config.sample_rate\n",
    "  ).numpy()\n",
    "  train_dataset = data['train'].map(transform=trs)\n",
    "  val_dataset = MusicDataset(data['validation'], transform=trs)\n",
    "  test_dataset = MusicDataset(data['test'], transform=trs)\n",
    "  return {\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset,\n",
    "    'genres': data['genres'],\n",
    "  }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_audio(dataset, map_fn):\n",
    "  \"\"\"\n",
    "  Applies a mapping function to the 'audio' column of a Hugging Face dataset.\n",
    "\n",
    "  Args:\n",
    "    dataset (datasets.Dataset): The input dataset.\n",
    "    map_fn (callable): The function to apply to each audio object.\n",
    "               It takes one argument (the audio object dict which contains 'array', 'sampling_rate', etc.)\n",
    "               and returns the transformed audio data.\n",
    "\n",
    "  Returns:\n",
    "    datasets.Dataset: The dataset with the transformed 'audio' column.\n",
    "  \"\"\"\n",
    "  return dataset.map(lambda example: {\"audio\": map_fn(example[\"audio\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 686\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered_ds.pop('genres')\n",
    "filtered_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15111916195e4dd2b7412d41fa4eb109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708a64a507134130b7537c81a615196a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d656c4d4a74ac3a8fef92dbc3481bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 686\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def trs(audio):\n",
    "  waveform = torchaudio.functional.resample(\n",
    "    waveform = torch.tensor(audio['array'], dtype=torch.float32), \n",
    "    orig_freq = audio['sampling_rate'], \n",
    "    new_freq=config.sample_rate\n",
    "  ).numpy()\n",
    "  audio[\"array\"] = waveform\n",
    "  audio[\"sampling_rate\"] = config.sample_rate\n",
    "  return audio\n",
    "  \n",
    "mapped_ds = map_audio(filtered_ds, trs)\n",
    "mapped_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': None, 'array': array([0.        , 0.        , 0.        , ..., 0.12091064, 0.16314697,\n",
      "       0.08712769], shape=(479626,)), 'sampling_rate': 16000}, 'title': 'Queen Of The Wires', 'url': 'http://freemusicarchive.org/music/Alec_K_Redfearn_and_the_Eyesores/The_Blind_Spot/Queen_Of_The_Wires', 'artist': 'Alec K. Redfearn & the Eyesores', 'composer': '', 'lyricist': '', 'publisher': '', 'genres': [58], 'tags': [], 'released': datetime.datetime(2008, 11, 26, 1, 44, 7), 'language': 'en', 'listens': 1299, 'artist_url': 'http://freemusicarchive.org/music/Alec_K_Redfearn_and_the_Eyesores/', 'artist_website': 'http://www.aleckredfearn.com', 'album_title': 'The Blind Spot', 'album_url': 'http://freemusicarchive.org/music/Alec_K_Redfearn_and_the_Eyesores/The_Blind_Spot/', 'license': 13, 'copyright': '', 'explicit': None, 'instrumental': 0, 'allow_commercial_use': 0, 'allow_derivatives': 0, 'require_attribution': 1, 'require_share_alike': 0}\n"
     ]
    }
   ],
   "source": [
    "for x in mapped_ds['train']:\n",
    "  print(x)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def aahhh(audio):\n",
    "#   audio[\"sampling_rate\"] = config.sample_rate\n",
    "#   return audio\n",
    "  \n",
    "# mapped_ds = map_audio(filtered_ds, aahhh)\n",
    "# mapped_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686 119 124\n",
      "{'audio': {'path': None, 'array': array([0.        , 0.        , 0.        , ..., 0.12091064, 0.16314697,\n",
      "       0.08712769], shape=(479626,)), 'sampling_rate': 16000}, 'title': 'Queen Of The Wires', 'url': 'http://freemusicarchive.org/music/Alec_K_Redfearn_and_the_Eyesores/The_Blind_Spot/Queen_Of_The_Wires', 'artist': 'Alec K. Redfearn & the Eyesores', 'composer': '', 'lyricist': '', 'publisher': '', 'genres': [58], 'tags': [], 'released': datetime.datetime(2008, 11, 26, 1, 44, 7), 'language': 'en', 'listens': 1299, 'artist_url': 'http://freemusicarchive.org/music/Alec_K_Redfearn_and_the_Eyesores/', 'artist_website': 'http://www.aleckredfearn.com', 'album_title': 'The Blind Spot', 'album_url': 'http://freemusicarchive.org/music/Alec_K_Redfearn_and_the_Eyesores/The_Blind_Spot/', 'license': 13, 'copyright': '', 'explicit': None, 'instrumental': 0, 'allow_commercial_use': 0, 'allow_derivatives': 0, 'require_attribution': 1, 'require_share_alike': 0}\n"
     ]
    }
   ],
   "source": [
    "print(len(mapped_ds['train']), len(mapped_ds['validation']), len(mapped_ds['test']))\n",
    "for x in mapped_ds['train']:\n",
    "  print(x)\n",
    "  break\n",
    "for x in mapped_ds['test']:\n",
    "  assert x['audio']['sampling_rate'] == 16000\n",
    "for x in mapped_ds['validation']:\n",
    "  assert x['audio']['sampling_rate'] == 16000\n",
    "for x in mapped_ds['train']:\n",
    "  assert x['audio']['sampling_rate'] == 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 686\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "mapped_ds['genres'] = Dataset.from_dict({\"any\": [12, 26, 58, 25]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 686\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "    genres: Dataset({\n",
       "        features: ['any'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d492d86c654444888149d9dfab60d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b33e79fa964aaf93f2bd9ac12efb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984d0b10ed8146838ef250f10c917a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e26019380ba4f9ab96877bf40d65661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mapped_ds.save_to_disk(\"rock_dataset_resampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(686, 119, 124)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_ds['train']), len(filtered_ds['validation']), len(filtered_ds['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MusicDataset' object has no attribute '_indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m filtered_sampled_ds = \u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m filtered_sampled_ds\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/arrow_dataset.py:940\u001b[39m, in \u001b[36mDataset.from_dict\u001b[39m\u001b[34m(cls, mapping, features, info, split)\u001b[39m\n\u001b[32m    938\u001b[39m     arrow_typed_mapping[col] = data\n\u001b[32m    939\u001b[39m mapping = arrow_typed_mapping\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m pa_table = \u001b[43mInMemoryTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    942\u001b[39m     info = DatasetInfo()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/table.py:758\u001b[39m, in \u001b[36mInMemoryTable.from_pydict\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    743\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pydict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *args, **kwargs):\n\u001b[32m    744\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[33;03m    Construct a Table from Arrow arrays or columns.\u001b[39;00m\n\u001b[32m    746\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    756\u001b[39m \u001b[33;03m        `datasets.table.Table`\u001b[39;00m\n\u001b[32m    757\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/pyarrow/table.pxi:1982\u001b[39m, in \u001b[36mpyarrow.lib._Tabular.from_pydict\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/pyarrow/table.pxi:6379\u001b[39m, in \u001b[36mpyarrow.lib._from_pydict\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/pyarrow/array.pxi:405\u001b[39m, in \u001b[36mpyarrow.lib.asarray\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/pyarrow/array.pxi:255\u001b[39m, in \u001b[36mpyarrow.lib.array\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/pyarrow/array.pxi:117\u001b[39m, in \u001b[36mpyarrow.lib._handle_arrow_array_protocol\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/arrow_writer.py:220\u001b[39m, in \u001b[36mTypedSequence.__arrow_array__\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# automatic type inference for custom objects\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.try_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m     data, \u001b[38;5;28mself\u001b[39m._inferred_type = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_infer_custom_type_and_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inferred_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;28mtype\u001b[39m = \u001b[38;5;28mself\u001b[39m.try_type \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trying_type \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.type\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/arrow_writer.py:192\u001b[39m, in \u001b[36mTypedSequence._infer_custom_type_and_encode\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.PIL_AVAILABLE \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mPIL\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys.modules:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mImage\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     non_null_idx, non_null_value = \u001b[43mfirst_non_null_non_empty_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(non_null_value, PIL.Image.Image):\n\u001b[32m    194\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [Image().encode_example(value) \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m data], Image()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/utils/py_utils.py:326\u001b[39m, in \u001b[36mfirst_non_null_non_empty_value\u001b[39m\u001b[34m(iterable)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfirst_non_null_non_empty_value\u001b[39m(iterable):\n\u001b[32m    325\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the index and the value of the first non-null non-empty value in the iterable. If all values are None or empty, return -1 as index.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/arrow_dataset.py:2378\u001b[39m, in \u001b[36mDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   2373\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Iterate through the examples.\u001b[39;00m\n\u001b[32m   2374\u001b[39m \n\u001b[32m   2375\u001b[39m \u001b[33;03m    If a formatting is set with [`Dataset.set_format`] rows will be returned with the\u001b[39;00m\n\u001b[32m   2376\u001b[39m \u001b[33;03m    selected format.\u001b[39;00m\n\u001b[32m   2377\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2378\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2379\u001b[39m         \u001b[38;5;66;03m# Fast iteration\u001b[39;00m\n\u001b[32m   2380\u001b[39m         \u001b[38;5;66;03m# Benchmark: https://gist.github.com/mariosasko/0248288a2e3a7556873969717c1fe52b (fast_iter_batch)\u001b[39;00m\n\u001b[32m   2381\u001b[39m         format_kwargs = \u001b[38;5;28mself\u001b[39m._format_kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2382\u001b[39m         formatter = get_formatter(\u001b[38;5;28mself\u001b[39m._format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n",
      "\u001b[31mAttributeError\u001b[39m: 'MusicDataset' object has no attribute '_indices'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "filtered_sampled_ds = Dataset.from_dict(make_datasets(filtered_ds, config=config))\n",
    "filtered_sampled_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling_rate:  44100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/core/transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (120904,)\n",
      "tensor torch.Size([120904])\n",
      "sampling_rate:  44100\n",
      "transform: (114143,)\n",
      "tensor torch.Size([114143])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (76901,)\n",
      "tensor torch.Size([76901])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (171601,)\n",
      "tensor torch.Size([171601])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (101116,)\n",
      "tensor torch.Size([101116])\n",
      "sampling_rate:  44100\n",
      "transform: (218164,)\n",
      "tensor torch.Size([218164])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (159634,)\n",
      "tensor torch.Size([159634])\n",
      "sampling_rate:  44100\n",
      "transform: (155134,)\n",
      "tensor torch.Size([155134])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (94957,)\n",
      "tensor torch.Size([94957])\n",
      "sampling_rate:  44100\n",
      "transform: (134404,)\n",
      "tensor torch.Size([134404])\n",
      "sampling_rate:  44100\n",
      "transform: (189544,)\n",
      "tensor torch.Size([189544])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (223282,)\n",
      "tensor torch.Size([223282])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (215536,)\n",
      "tensor torch.Size([215536])\n",
      "sampling_rate:  44100\n",
      "transform:"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 285, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [65536] at entry 0 and [120904] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dls = make_dataloaders(filtered_ds, config)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdls\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1515\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1513\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1514\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1515\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1550\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1548\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 285, in collate_numpy_array_fn\n    return collate([torch.as_tensor(b) for b in batch], collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 155, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 272, in collate_tensor_fn\n    return torch.stack(batch, 0, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [65536] at entry 0 and [120904] at entry 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (82801,)\n",
      "tensor torch.Size([82801])\n",
      "sampling_rate:  44100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (157999,)\n",
      "tensor torch.Size([157999])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (96663,)\n",
      "tensor torch.Size([96663])\n",
      "sampling_rate:  44100\n",
      "transform: (93734,)\n",
      "tensor torch.Size([93734])\n",
      "sampling_rate:  44100\n",
      "transform: (188149,)\n",
      "tensor torch.Size([188149])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (210316,)\n",
      "tensor torch.Size([210316])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (130912,)\n",
      "tensor torch.Size([130912])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (169014,)\n",
      "tensor torch.Size([169014])\n",
      "sampling_rate:  44100\n",
      "transform: (220984,)\n",
      "tensor torch.Size([220984])\n",
      "sampling_rate:  44100\n",
      "transform: (130164,)\n",
      "tensor torch.Size([130164])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (99046,)\n",
      "tensor torch.Size([99046])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (220225,)\n",
      "tensor torch.Size([220225])\n",
      "sampling_rate:  44100\n",
      "transform: (82531,)\n",
      "tensor torch.Size([82531])\n",
      "sampling_rate:  44100\n",
      "transform: (144553,)\n",
      "tensor torch.Size([144553])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (104665,)\n",
      "tensor torch.Size([104665])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (116102,)\n",
      "tensor torch.Size([116102])\n",
      "sampling_rate:  44100\n",
      "transform: (135594,)\n",
      "tensor torch.Size([135594])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n",
      "sampling_rate:  44100\n",
      "transform: (65536,)\n",
      "tensor torch.Size([65536])\n"
     ]
    }
   ],
   "source": [
    "dls = make_dataloaders(filtered_ds, config)\n",
    "for x in dls['train']:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d86c6d46db439cb7a1423119e2d8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57c23c4a26240299750a90328c7f27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92c0d7252204405b6b64a2337a8b581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_ds.save_to_disk(\"filtered_dataset_by_artist_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
      "        num_rows: 653\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
      "        num_rows: 152\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
      "        num_rows: 124\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "def load_dataloader(path):\n",
    "  ds = load_from_disk(path)\n",
    "  ds['train'].set_transform()\n",
    "\n",
    "\n",
    "pds = load_from_disk(\"filtered_dataset_by_artist_split\")\n",
    "print(f\"Loaded dataset: {pds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_from_disk(\"filtered_fma_rock\")\n",
    "from datasets import Dataset\n",
    "pds['genres'] = Dataset.from_dict({\"any\": [26, 58, 25]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 653\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 152\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "        num_rows: 124\n",
       "    })\n",
       "    genres: Dataset({\n",
       "        features: ['any'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e085f1e302eb4de280b0aebaaace3cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/653 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2643ac17fb2489d839f1bc4bcad6dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a111390458945bd840933135266a42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb753de7740141ebaf61bfbb121755c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pds.save_to_disk(\"filter_fma_rock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'],\n",
       "    num_rows: 653\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pds = load_from_disk(\"filtered_dataset_by_artist_split\")\n",
    "pds['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [1,2,3]\n",
    "xs + [4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch_audiomentations as taug\n",
    "import torchaudio as ta\n",
    "import audiomentations as aug\n",
    "import audiomentations as taug\n",
    "from datasets import Features, Audio\n",
    "\n",
    "def optional(x, bool):\n",
    "  if bool:\n",
    "    return [x]\n",
    "  else:\n",
    "    return []\n",
    "\n",
    "# samples_per_s = 44100\n",
    "def audio_augmentations(sample_size, sample_rate, use_train_augs = False):\n",
    "  output_type = 'tensor'\n",
    "  augmentation = aug.Compose(\n",
    "    transforms=\n",
    "      [aug.AdjustDuration(duration_samples=sample_size, p=1.0)]\n",
    "      +\n",
    "      optional(\n",
    "        aug.TimeStretch(\n",
    "            min_rate=0.8,\n",
    "            max_rate=1.25,\n",
    "            leave_length_unchanged=True,\n",
    "            p=0.8\n",
    "        ),\n",
    "        use_train_augs)\n",
    "      + optional(\n",
    "        aug.OneOf(\n",
    "          transforms=[\n",
    "              taug.LowPassFilter(\n",
    "                  min_cutoff_freq=500.0,\n",
    "                  max_cutoff_freq=2000.0,\n",
    "                  p=1.0,\n",
    "                  # output_type=output_type\n",
    "              ),\n",
    "              taug.HighPassFilter(\n",
    "                  min_cutoff_freq=100.0,\n",
    "                  max_cutoff_freq=2400.0,\n",
    "                  p=1.0,\n",
    "                  # output_type=output_type\n",
    "              ),\n",
    "              taug.BandPassFilter(\n",
    "                  # min_center_frequency=200.0,\n",
    "                  # max_center_frequency=4000.0,\n",
    "                  min_center_freq=200.0,\n",
    "                  max_center_freq=4000.0,\n",
    "                  p=1.0,\n",
    "                  # output_type=output_type\n",
    "              ),\n",
    "              aug.ClippingDistortion(min_percentile_threshold=0.01,\n",
    "                  max_percentile_threshold=0.99,\n",
    "                  p=0.5),\n",
    "              aug.Limiter(\n",
    "                min_threshold_db=-16.0,\n",
    "                max_threshold_db=-6.0,\n",
    "                threshold_mode=\"relative_to_signal_peak\",\n",
    "                p=1.0,\n",
    "              ),\n",
    "              taug.PitchShift(\n",
    "                # min_transpose_semitones=-5.0,\n",
    "                # max_transpose_semitones=5.0,\n",
    "                min_semitones=-5.0,\n",
    "                max_semitones=5.0,\n",
    "                # sample_rate=sample_rate,\n",
    "                p=1.0,\n",
    "                # output_type=output_type\n",
    "              ),\n",
    "          ],\n",
    "          p=1.0\n",
    "        ),\n",
    "        use_train_augs)\n",
    "      +\n",
    "      optional(\n",
    "        aug.SomeOf( transforms=[\n",
    "          aug.TanhDistortion(\n",
    "            min_distortion=0.01,\n",
    "            max_distortion=0.15,\n",
    "            p=0.8\n",
    "          ),\n",
    "          aug.RepeatPart(mode=\"insert\", p=0.8),\n",
    "        ],\n",
    "        num_transforms=(0,None),\n",
    "        ),\n",
    "        use_train_augs)\n",
    "      +\n",
    "      optional(\n",
    "        aug.Gain(\n",
    "          min_gain_db=-15.0,\n",
    "          max_gain_db=5.0,\n",
    "          # p=0.5,\n",
    "          p=0.5,\n",
    "        )\n",
    "        , use_train_augs)\n",
    "      +\n",
    "      # optional(\n",
    "      #   taug.ShuffleChannels(p=0.5, \n",
    "      #     # output_type=output_type\n",
    "      #     ),\n",
    "      #   use_train_augs)\n",
    "      # +\n",
    "      \n",
    "      # taug.OneOf(transforms=[\n",
    "      #   taug.PeakNormalization(apply_to=\"only_too_loud_sounds\", p=1.0, output_type=output_type),\n",
    "        # [taug.PeakNormalization(apply_to=\"all\", p=1.0\n",
    "        #                         # , output_type=output_type\n",
    "        #                         )]\n",
    "        [aug.Normalize(apply_to=\"only_too_loud_sounds\", p=1.0)]\n",
    "      # ], output_type=output_type),\n",
    "      +\n",
    "      optional(\n",
    "        aug.PolarityInversion(p=0.5),\n",
    "      use_train_augs)\n",
    "  )\n",
    "\n",
    "  # def transforms(examples):\n",
    "  #   # print(len(examples['audio']), \"audio samples\")\n",
    "  #   x = [augmentation(aud['array'], sample_rate=aud['sampling_rate']) for aud in examples['audio']]\n",
    "  #   examples[\"audio\"] = x\n",
    "  #   print(\"Transformed audio samples:\", [aud.shape for aud in examples['audio']])\n",
    "  #   return examples\n",
    "  \n",
    "  # return transforms, augmentation\n",
    "  return augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable Compose object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[168]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m sample_size = \u001b[32m65536\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# taug, aug = audio_augmentations(sample_size, sample_rate, use_train_augs = False)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m taug, aug = audio_augmentations(sample_size, sample_rate, use_train_augs = \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable Compose object"
     ]
    }
   ],
   "source": [
    "sample_rate = 16000\n",
    "sample_size = 65536\n",
    "# taug, aug = audio_augmentations(sample_size, sample_rate, use_train_augs = False)\n",
    "taug, aug = audio_augmentations(sample_size, sample_rate, use_train_augs = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "pds = load_from_disk(\"./filter_fma_rock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 653\n",
      "Val dataset size: 152\n",
      "Test dataset size: 124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "  def __init__(self, hf_dataset, transform=None):\n",
    "    self.dataset = hf_dataset\n",
    "    self.transform = transform\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.dataset)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    item = self.dataset[idx]\n",
    "    audio = item['audio']['array']\n",
    "    if self.transform:\n",
    "      audio = self.transform(audio, sample_rate=item['audio']['sampling_rate'])\n",
    "    audio_tensor = torch.tensor(audio, dtype=torch.float32)\n",
    "    return audio_tensor\n",
    "\n",
    "test_transformation = audio_augmentations(sample_size, sample_rate, use_train_augs = False)\n",
    "train_dataset = MusicDataset(pds['train'], transform=audio_augmentations(sample_size, sample_rate, use_train_augs = True))\n",
    "val_dataset = MusicDataset(pds['validation'], transform=test_transformation)\n",
    "test_dataset = MusicDataset(pds['test'], transform=test_transformation)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Val dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(data, num_workers=4, batch_size=32):\n",
    "  test_transformation = audio_augmentations(sample_size, sample_rate, use_train_augs = False)\n",
    "  train_dataset = MusicDataset(data['train'], transform=audio_augmentations(sample_size, sample_rate, use_train_augs = True))\n",
    "  val_dataset = MusicDataset(data['validation'], transform=test_transformation)\n",
    "  test_dataset = MusicDataset(data['test'], transform=test_transformation)\n",
    "\n",
    "  dl = lambda ds: data.DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "  return {\n",
    "    'train': dl(train_dataset),\n",
    "    'validation': dl(val_dataset),\n",
    "    'test': dl(test_dataset)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "# pds['train'].set_transform(aug)\n",
    "# pds['train'].set_format(\"torch\", columns=[\"audio\"])\n",
    "\n",
    "train_dl = data.DataLoader(train_dataset, 16, shuffle=False, num_workers=1) # num_workers=2 is enough for 2 shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/core/transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Failed to import cylimiter. Maybe it is not installed? To install the optional cylimiter dependency of audiomentations, run `pip install cylimiter` or `pip install audiomentations[extras]`\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "Caught ModuleNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/nix-shell-6340-0/ipykernel_8777/3023671263.py\", line 16, in __getitem__\n    audio = self.transform(audio, sample_rate=item['audio']['sampling_rate'])\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/core/composition.py\", line 130, in __call__\n    samples = transform(samples, sample_rate)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/core/composition.py\", line 276, in __call__\n    return self.transforms[self.transform_index](*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/core/transforms_interface.py\", line 138, in __call__\n    return self.apply(samples, sample_rate)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/augmentations/limiter.py\", line 119, in apply\n    from cylimiter import Limiter as CyLimiter\nModuleNotFoundError: No module named 'cylimiter'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[183]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1515\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1513\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1514\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1515\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1550\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1548\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: Caught ModuleNotFoundError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/nix-shell-6340-0/ipykernel_8777/3023671263.py\", line 16, in __getitem__\n    audio = self.transform(audio, sample_rate=item['audio']['sampling_rate'])\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/core/composition.py\", line 130, in __call__\n    samples = transform(samples, sample_rate)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/core/composition.py\", line 276, in __call__\n    return self.transforms[self.transform_index](*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/core/transforms_interface.py\", line 138, in __call__\n    return self.apply(samples, sample_rate)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/augmentations/limiter.py\", line 119, in apply\n    from cylimiter import Limiter as CyLimiter\nModuleNotFoundError: No module named 'cylimiter'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import cylimiter. Maybe it is not installed? To install the optional cylimiter dependency of audiomentations, run `pip install cylimiter` or `pip install audiomentations[extras]`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to import cylimiter. Maybe it is not installed? To install the optional cylimiter dependency of audiomentations, run `pip install cylimiter` or `pip install audiomentations[extras]`\n"
     ]
    }
   ],
   "source": [
    "for i in train_dl:\n",
    "  print(i.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0445, -0.0464, -0.0459,  ..., -0.0213, -0.0375, -0.0555])\n"
     ]
    }
   ],
   "source": [
    "for i in val_dataset:\n",
    "  print(i)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[162]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mauggg\u001b[39m(aud):\n\u001b[32m      2\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m aug(aud[\u001b[33m'\u001b[39m\u001b[33marray\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mpds\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m(auggg)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "def auggg(aud):\n",
    "  return aug(aud['array'])\n",
    "pds['train']['audio'].map(auggg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed audio samples: [(65536,)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/audiomentations/core/transforms_interface.py:107: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'audio_sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[146]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m pds[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m].set_transform(aug)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pds[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m      4\u001b[39m   \u001b[38;5;66;03m# print(batch)\u001b[39;00m\n\u001b[32m      5\u001b[39m   \u001b[38;5;66;03m# print(batch['audio'])\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maudio_sample\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m]))\n\u001b[32m      7\u001b[39m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'audio_sample'"
     ]
    }
   ],
   "source": [
    "pds['train'] = pds['train'].with_format(\"torch\", columns=[\"audio\"])\n",
    "pds['train'].set_transform(aug)\n",
    "for batch in pds['train']:\n",
    "  # print(batch)\n",
    "  # print(batch['audio'])\n",
    "  print(type(batch['audio_sample'][0]))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3068763972.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[136]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpds['train'].\u001b[39m\n                 ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pds['train']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "pds['train'].set_transform(aug)\n",
    "pds['train'].set_format(\"torch\", columns=[\"audio\"])\n",
    "\n",
    "train_dl = data.DataLoader(pds['train'], 10, shuffle=True, num_workers=1, drop_last=True, col) # num_workers=2 is enough for 2 shards\n",
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'bool' object is not callable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[157]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1515\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1513\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1514\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1515\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1550\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1548\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mTypeError\u001b[39m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/zmrocze/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\nTypeError: 'bool' object is not callable\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dl:\n",
    "  print(batch)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_disk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pds = \u001b[43mload_from_disk\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mfiltered_dataset_by_artist_split\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# pds = pds.with_format(\"torch\")\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# pds['train'].set_transform(\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#   audio_augmentations(sample_size=65536, sample_rate=44100)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'load_from_disk' is not defined"
     ]
    }
   ],
   "source": [
    "pds = load_from_disk(\"filtered_dataset_by_artist_split\")\n",
    "# pds = pds.with_format(\"torch\")\n",
    "# pds['train'].set_transform(\n",
    "#   audio_augmentations(sample_size=65536, sample_rate=44100)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "train_dl = data.DataLoader(pds['train'], 10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1321967] at entry 0 and [1323119] at entry 4",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m first_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(first_batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:172\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m    171\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    176\u001b[39m         }\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:172\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections.abc.MutableMapping):\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[32m    169\u001b[39m     clone = copy.copy(elem)\n\u001b[32m    170\u001b[39m     clone.update(\n\u001b[32m    171\u001b[39m         {\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[32m    176\u001b[39m         }\n\u001b[32m    177\u001b[39m     )\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[39m, in \u001b[36mcollate_tensor_fn\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    270\u001b[39m     storage = elem._typed_storage()._new_shared(numel, device=elem.device)\n\u001b[32m    271\u001b[39m     out = elem.new(storage).resize_(\u001b[38;5;28mlen\u001b[39m(batch), *\u001b[38;5;28mlist\u001b[39m(elem.size()))\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: stack expects each tensor to be equal size, but got [1321967] at entry 0 and [1323119] at entry 4"
     ]
    }
   ],
   "source": [
    "first_batch = next(iter(train_dl))\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio', 'title', 'url', 'artist', 'composer', 'lyricist', 'publisher', 'genres', 'tags', 'released', 'language', 'listens', 'artist_url', 'artist_website', 'album_title', 'album_url', 'license', 'copyright', 'explicit', 'instrumental', 'allow_commercial_use', 'allow_derivatives', 'require_attribution', 'require_share_alike'])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Get first batch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m first_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(first_batch)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Extract the first audio sample from the batch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/arrow_dataset.py:2781\u001b[39m, in \u001b[36mDataset.__getitems__\u001b[39m\u001b[34m(self, keys)\u001b[39m\n\u001b[32m   2779\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: \u001b[38;5;28mlist\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m   2780\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2781\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2782\u001b[39m     n_examples = \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[32m   2783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch.items()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/arrow_dataset.py:2777\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2775\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[32m   2776\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2777\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/arrow_dataset.py:2762\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2760\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2761\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2762\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2763\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2764\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/formatting/formatting.py:653\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    651\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/formatting/formatting.py:410\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.12/site-packages/datasets/formatting/formatting.py:536\u001b[39m, in \u001b[36mCustomFormatter.format_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    534\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.python_arrow_extractor().extract_batch(pa_table)\n\u001b[32m    535\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_batch(batch)\n\u001b[32m--> \u001b[39m\u001b[32m536\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36maudio_augmentations.<locals>.transforms\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransforms\u001b[39m(examples):\n\u001b[32m     81\u001b[39m   \u001b[38;5;28mprint\u001b[39m(examples.keys())\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m)\n\u001b[32m     83\u001b[39m   examples[\u001b[33m\"\u001b[39m\u001b[33maudio_sample\u001b[39m\u001b[33m\"\u001b[39m] = augmentation(examples[\u001b[33m'\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m'\u001b[39m], sample_rate=sample_rate)\n\u001b[32m     84\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m examples\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get first batch from train_dl and play the first audio sample\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Get first batch\n",
    "first_batch = next(iter(train_dl))\n",
    "\n",
    "print(first_batch)\n",
    "\n",
    "# Extract the first audio sample from the batch\n",
    "first_audio = first_batch['audio_sample'][0]  # Get first sample from batch\n",
    "sample_rate = 44100  # Using the sample rate from your augmentation function\n",
    "\n",
    "print(first_audio)\n",
    "\n",
    "# Play the audio\n",
    "ipd.Audio(first_audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pds = pds.set_transform(transforms)\n",
    "pds['train'].set_transform(transforms)\n",
    "pds\n",
    "# fds = fds.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '000002.mp3',\n",
       "  'array': array([ 0.        ,  0.        ,  0.        , ..., -0.24734139,\n",
       "         -0.19890546, -0.10515908], shape=(1321967,)),\n",
       "  'sampling_rate': 44100},\n",
       " 'title': 'Food',\n",
       " 'url': 'http://freemusicarchive.org/music/AWOL/AWOL_-_A_Way_Of_Life/Food',\n",
       " 'artist': 'AWOL',\n",
       " 'composer': '',\n",
       " 'lyricist': '',\n",
       " 'publisher': '',\n",
       " 'genres': [70],\n",
       " 'tags': [],\n",
       " 'released': datetime.datetime(2008, 11, 26, 1, 48, 12),\n",
       " 'language': 'en',\n",
       " 'listens': 1293,\n",
       " 'artist_url': 'http://freemusicarchive.org/music/AWOL/',\n",
       " 'artist_website': 'http://www.AzillionRecords.blogspot.com',\n",
       " 'album_title': 'AWOL - A Way Of Life',\n",
       " 'album_url': 'http://freemusicarchive.org/music/AWOL/AWOL_-_A_Way_Of_Life/',\n",
       " 'license': 18,\n",
       " 'copyright': '',\n",
       " 'explicit': 1,\n",
       " 'instrumental': 0,\n",
       " 'allow_commercial_use': 0,\n",
       " 'allow_derivatives': 1,\n",
       " 'require_attribution': 1,\n",
       " 'require_share_alike': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d22f80f5154b79a75efcb2399a2d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651353b4e53e4a03a1d9aa062f6a3588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio-classification\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdima806/music_genres_classification\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/transformers/pipelines/__init__.py:942\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    941\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    952\u001b[39m model_config = model.config\n\u001b[32m    953\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/transformers/pipelines/base.py:292\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m     logger.warning(\n\u001b[32m    287\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    288\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to load the model with Tensorflow.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    289\u001b[39m     )\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    294\u001b[39m         model = model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/transformers/modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/transformers/modeling_utils.py:4422\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4413\u001b[39m     gguf_file\n\u001b[32m   4414\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4415\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4416\u001b[39m ):\n\u001b[32m   4417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4418\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4419\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4420\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4422\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4423\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4424\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4429\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4435\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4439\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4441\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4442\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/transformers/modeling_utils.py:1024\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1010\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m   1011\u001b[39m     cached_file_kwargs = {\n\u001b[32m   1012\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m   1013\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m: commit_hash,\n\u001b[32m   1023\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1026\u001b[39m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[32m   1027\u001b[39m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[32m   1028\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[32m   1029\u001b[39m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/transformers/utils/hub.py:312\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    255\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    256\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    257\u001b[39m     **kwargs,\n\u001b[32m    258\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    259\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    261\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    311\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    313\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/transformers/utils/hub.py:470\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    469\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    485\u001b[39m         snapshot_download(\n\u001b[32m    486\u001b[39m             path_or_repo_id,\n\u001b[32m    487\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    496\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    497\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/huggingface_hub/file_download.py:1008\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    989\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    990\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1006\u001b[39m     )\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/huggingface_hub/file_download.py:1161\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1158\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1160\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1173\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1174\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/huggingface_hub/file_download.py:1725\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1718\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1719\u001b[39m             logger.warning(\n\u001b[32m   1720\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo, but the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhf_xet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1721\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFalling back to regular HTTP download. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1722\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1723\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1725\u001b[39m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1728\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1729\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1730\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1731\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1732\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1734\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1735\u001b[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/huggingface_hub/file_download.py:494\u001b[39m, in \u001b[36mhttp_get\u001b[39m\u001b[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[39m\n\u001b[32m    492\u001b[39m new_resume_size = resume_size\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDOWNLOAD_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep-alive new chunks\u001b[39;49;00m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/requests/models.py:820\u001b[39m, in \u001b[36mResponse.iter_content.<locals>.generate\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.raw, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raw.stream(chunk_size, decode_content=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/urllib3/response.py:1066\u001b[39m, in \u001b[36mHTTPResponse.stream\u001b[39m\u001b[34m(self, amt, decode_content)\u001b[39m\n\u001b[32m   1064\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1065\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m._fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1068\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[32m   1069\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/urllib3/response.py:955\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt, decode_content, cache_content)\u001b[39m\n\u001b[32m    952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) >= amt:\n\u001b[32m    953\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decoded_buffer.get(amt)\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    957\u001b[39m flush_decoder = amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._decoded_buffer) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/urllib3/response.py:879\u001b[39m, in \u001b[36mHTTPResponse._raw_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    876\u001b[39m fp_closed = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._fp, \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._error_catcher():\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt != \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[32m    882\u001b[39m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    887\u001b[39m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[32m    888\u001b[39m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[32m    889\u001b[39m         \u001b[38;5;28mself\u001b[39m._fp.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/site-packages/urllib3/response.py:862\u001b[39m, in \u001b[36mHTTPResponse._fp_read\u001b[39m\u001b[34m(self, amt, read1)\u001b[39m\n\u001b[32m    859\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read1()\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    861\u001b[39m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/studia/uwr/sem2/nn/nn_projekt/.mamba/envs/my_env1/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"audio-classification\", model=\"dima806/music_genres_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=config.train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (941091042.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mds.\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# def filter_by_genres(dataset, genre_ids):\n",
    "#   def contains_genres(example):\n",
    "#     print(example)\n",
    "#     if example is None:\n",
    "#       return False\n",
    "#     return False\n",
    "#     return all(genre_id in example['genres'] for genre_id in genre_ids)\n",
    "  \n",
    "#   return .filter(contains_genres)\n",
    "\n",
    "ds\n",
    "\n",
    "\n",
    "filter_by_genres(ds, [26, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using polars (dataframe-like) interface for ds hugging face dataset perform given tasks: split data into train and test, putting "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
